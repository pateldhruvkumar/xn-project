{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP Analysis on Summary Notes\n",
        "\n",
        "This notebook performs comprehensive Natural Language Processing analysis on the Summary Notes field from the fiscal year data.\n",
        "\n",
        "**Analysis includes:**\n",
        "- Exploratory Data Analysis (EDA)\n",
        "- Text Preprocessing (tokenization, stopwords removal, lemmatization)\n",
        "- Text Encoding (Bag of Words, TF-IDF)\n",
        "- Pre-trained Word Embeddings (GloVe)\n",
        "- Document Clustering\n",
        "- Topic Modeling with LDA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Packages & Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# NLP libraries\n",
        "import ssl\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Sklearn for vectorization and topic modeling\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Yellowbrick for visualizations\n",
        "from yellowbrick.text import FreqDistVisualizer, TSNEVisualizer\n",
        "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style and seed\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "np.random.seed(123)\n",
        "\n",
        "print(\"✓ All packages loaded successfully\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update this path for your environment\n",
        "# For Kaggle: file_path = \"/kaggle/input/your-dataset/FY19_to_FY23_Cleaned.xlsx\"\n",
        "file_path = \"D:/Projects/xn-project/dataset/FY19_to_FY25_Final.xlsx\"\n",
        "raw_df = pd.read_excel(file_path)\n",
        "\n",
        "# Create document dataframe\n",
        "docs = pd.DataFrame({\n",
        "    'doc_id': [f'D{i+1}' for i in range(len(raw_df))],\n",
        "    'text': raw_df['Summary Notes']\n",
        "})\n",
        "\n",
        "# Filter out missing/empty text\n",
        "docs = docs[docs['text'].notna() & (docs['text'].str.strip() != '')]\n",
        "docs = docs.reset_index(drop=True)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\" * 70)\n",
        "print(docs.head())\n",
        "print(f\"\\nDataset shape: {docs.shape}\")\n",
        "print(\"\\nDataframe info:\")\n",
        "print(docs.info())\n",
        "print(\"=\" * 70 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. EDA (Exploratory Data Analysis)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"EXPLORATORY DATA ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "def count_sentences(text):\n",
        "    \"\"\"Count sentences in text\"\"\"\n",
        "    return len(sent_tokenize(str(text)))\n",
        "\n",
        "def count_words(text):\n",
        "    \"\"\"Count words in text\"\"\"\n",
        "    return len(word_tokenize(str(text)))\n",
        "\n",
        "# Add text statistics\n",
        "docs['n_sent'] = docs['text'].apply(count_sentences)\n",
        "docs['n_word'] = docs['text'].apply(count_words)\n",
        "docs['n_char'] = docs['text'].apply(len)\n",
        "\n",
        "print(\"\\nText Statistics:\")\n",
        "print(docs[['n_sent', 'n_word', 'n_char']].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top tokens (raw, lowercased)\n",
        "all_words = []\n",
        "for text in docs['text']:\n",
        "    words = word_tokenize(text.lower())\n",
        "    all_words.extend(words)\n",
        "\n",
        "word_freq = Counter(all_words)\n",
        "top_tokens = pd.DataFrame(word_freq.most_common(10), columns=['token', 'count'])\n",
        "print(\"\\nTop 10 tokens (raw, lowercased):\")\n",
        "print(top_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize top tokens with Yellowbrick\n",
        "from yellowbrick.text import FreqDistVisualizer\n",
        "\n",
        "# Prepare text for Yellowbrick\n",
        "all_docs_text = docs['text'].tolist()\n",
        "\n",
        "# Create vectorizer and fit\n",
        "vectorizer = CountVectorizer(lowercase=True, stop_words=None)\n",
        "docs_vectorized = vectorizer.fit_transform(all_docs_text)\n",
        "\n",
        "# Use Yellowbrick FreqDistVisualizer\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "visualizer = FreqDistVisualizer(\n",
        "    features=vectorizer.get_feature_names_out(),\n",
        "    n=25,\n",
        "    ax=ax\n",
        ")\n",
        "visualizer.fit(docs_vectorized)\n",
        "visualizer.finalize()\n",
        "plt.title(\"Top 25 Tokens (Raw, Lowercased) - Yellowbrick Visualization\", fontsize=14, pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 70 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Text Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"TEXT PREPROCESSING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Tokenize, remove stopwords, and lemmatize\"\"\"\n",
        "    # Tokenize and lowercase\n",
        "    tokens = word_tokenize(str(text).lower())\n",
        "    \n",
        "    # Remove stopwords and non-alphabetic tokens\n",
        "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    \n",
        "    # Lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing\n",
        "print(\"Preprocessing documents...\")\n",
        "docs['tokens'] = docs['text'].apply(preprocess_text)\n",
        "docs['processed_text'] = docs['tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Create token dataframe for analysis\n",
        "tokens_df = []\n",
        "for idx, row in docs.iterrows():\n",
        "    for token in row['tokens']:\n",
        "        tokens_df.append({'doc_id': row['doc_id'], 'word': token})\n",
        "tokens_df = pd.DataFrame(tokens_df)\n",
        "\n",
        "print(f\"Total tokens after preprocessing: {len(tokens_df)}\")\n",
        "print(f\"Unique tokens: {tokens_df['word'].nunique()}\")\n",
        "print(\"\\nSample preprocessed tokens:\")\n",
        "print(tokens_df.head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize top tokens after preprocessing with Yellowbrick\n",
        "vectorizer_clean = CountVectorizer()\n",
        "docs_clean_vectorized = vectorizer_clean.fit_transform(docs['processed_text'])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "visualizer = FreqDistVisualizer(\n",
        "    features=vectorizer_clean.get_feature_names_out(),\n",
        "    n=25,\n",
        "    ax=ax,\n",
        "    color='green'\n",
        ")\n",
        "visualizer.fit(docs_clean_vectorized)\n",
        "visualizer.finalize()\n",
        "plt.title(\"Top 25 Tokens (After Preprocessing) - Yellowbrick Visualization\", fontsize=14, pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 70 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Text Encoding: Bag of Words & TF-IDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"TEXT ENCODING: BAG OF WORDS & TF-IDF\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Bag of Words\n",
        "bow = tokens_df.groupby(['doc_id', 'word']).size().reset_index(name='n')\n",
        "bow = bow.sort_values('n', ascending=False)\n",
        "\n",
        "print(\"\\nBag of Words (top 20 term frequencies):\")\n",
        "print(bow.head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(docs['processed_text'])\n",
        "\n",
        "# Create TF-IDF dataframe\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_df = []\n",
        "for idx, doc_id in enumerate(docs['doc_id']):\n",
        "    tfidf_scores = tfidf_matrix[idx].toarray()[0]\n",
        "    for word_idx, score in enumerate(tfidf_scores):\n",
        "        if score > 0:\n",
        "            tfidf_df.append({\n",
        "                'doc_id': doc_id,\n",
        "                'word': feature_names[word_idx],\n",
        "                'tf_idf': score\n",
        "            })\n",
        "\n",
        "tfidf_df = pd.DataFrame(tfidf_df).sort_values('tf_idf', ascending=False)\n",
        "\n",
        "print(\"\\nTop TF-IDF terms per document (first 20 rows):\")\n",
        "top_tfidf_per_doc = tfidf_df.groupby('doc_id').head(5)\n",
        "print(top_tfidf_per_doc.head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot top TF-IDF terms overall\n",
        "top_tfidf_words = tfidf_df.groupby('word')['tf_idf'].sum().sort_values(ascending=False).head(15)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.barh(range(len(top_tfidf_words)), top_tfidf_words.values, color='steelblue')\n",
        "plt.yticks(range(len(top_tfidf_words)), top_tfidf_words.index)\n",
        "plt.xlabel('Total TF-IDF Score', fontsize=12)\n",
        "plt.ylabel('Terms', fontsize=12)\n",
        "plt.title('Top 15 Terms by Total TF-IDF', fontsize=14, pad=20)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 70 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Pre-trained Word Embeddings (GloVe 6B, 50d)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"PRE-TRAINED WORD EMBEDDINGS (GloVe)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# For Kaggle, update path to: \"/kaggle/input/glove-dataset/glove.6B.50d.txt\"\n",
        "emb_path = Path(\"data/glove.6B.50d.txt\")\n",
        "\n",
        "if not emb_path.exists():\n",
        "    print(\"\\n⚠️  Missing 'data/glove.6B.50d.txt'\")\n",
        "    print(\"Please download GloVe 6B from: https://nlp.stanford.edu/projects/glove/\")\n",
        "    print(\"Or add GloVe dataset in Kaggle via '+ Add Data'\\n\")\n",
        "    embeddings = None\n",
        "else:\n",
        "    print(\"Loading GloVe embeddings (this may take a moment)...\")\n",
        "    \n",
        "    # Load GloVe embeddings\n",
        "    embeddings = {}\n",
        "    with open(emb_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    \n",
        "    print(f\"✓ Embeddings loaded: {len(embeddings)} words × {len(next(iter(embeddings.values())))} dims\\n\")\n",
        "    \n",
        "    # Cosine similarity function\n",
        "    from scipy.spatial.distance import cosine\n",
        "    \n",
        "    def cosine_similarity_top_n(embeddings, term, topn=5):\n",
        "        \"\"\"Find most similar words using cosine similarity\"\"\"\n",
        "        term = term.lower()\n",
        "        if term not in embeddings:\n",
        "            print(f\"Term '{term}' not found in embeddings\")\n",
        "            return pd.DataFrame({'term': [], 'similarity': []})\n",
        "        \n",
        "        term_vec = embeddings[term]\n",
        "        similarities = {}\n",
        "        \n",
        "        for word, vec in embeddings.items():\n",
        "            if word != term:\n",
        "                sim = 1 - cosine(term_vec, vec)\n",
        "                similarities[word] = sim\n",
        "        \n",
        "        top_similar = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:topn]\n",
        "        return pd.DataFrame(top_similar, columns=['term', 'similarity'])\n",
        "    \n",
        "    print(\"Nearest neighbors (pre-trained GloVe 50d):\\n\")\n",
        "    \n",
        "    # Test with multiple relevant terms\n",
        "    test_terms = ['email', 'meeting', 'review', 'project']\n",
        "    for term in test_terms:\n",
        "        if term in embeddings:\n",
        "            print(f\"\\nTop 5 similar words to '{term}':\")\n",
        "            print(cosine_similarity_top_n(embeddings, term, 5))\n",
        "\n",
        "print(\"=\" * 70 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Document Clustering with Yellowbrick\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"DOCUMENT CLUSTERING VISUALIZATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Use TF-IDF matrix for clustering\n",
        "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
        "\n",
        "# K-Elbow Visualizer to find optimal k\n",
        "print(\"\\nFinding optimal number of clusters using Elbow method...\")\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "model = KMeans(random_state=42)\n",
        "visualizer = KElbowVisualizer(model, k=(2, 12), ax=ax, timings=False)\n",
        "visualizer.fit(tfidf_matrix.toarray())\n",
        "visualizer.finalize()\n",
        "plt.title(\"K-Elbow Visualizer for Optimal Clusters\", fontsize=14, pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "optimal_k = visualizer.elbow_value_\n",
        "if optimal_k is None:\n",
        "    optimal_k = 5\n",
        "print(f\"Suggested optimal k: {optimal_k}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Silhouette Visualizer\n",
        "print(f\"\\nVisualizing Silhouette scores for k={optimal_k}...\")\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "model = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "visualizer = SilhouetteVisualizer(model, colors='yellowbrick', ax=ax)\n",
        "visualizer.fit(tfidf_matrix.toarray())\n",
        "visualizer.finalize()\n",
        "plt.title(f\"Silhouette Plot for K-Means Clustering (k={optimal_k})\", fontsize=14, pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Topic Modeling (LDA, k = 7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"TOPIC MODELING WITH LDA\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create document-term matrix for LDA\n",
        "count_vectorizer = CountVectorizer(max_features=1000, min_df=2)\n",
        "dtm = count_vectorizer.fit_transform(docs['processed_text'])\n",
        "\n",
        "print(f\"\\nDocument-Term Matrix shape: {dtm.shape}\")\n",
        "print(\"Sample of DTM (first 2 docs, first 5 terms):\")\n",
        "print(dtm[:2, :5].toarray())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit LDA model\n",
        "n_topics = 7\n",
        "print(f\"\\nFitting LDA model with {n_topics} topics...\")\n",
        "\n",
        "lda_model = LatentDirichletAllocation(\n",
        "    n_components=n_topics,\n",
        "    random_state=42,\n",
        "    max_iter=20,\n",
        "    learning_method='batch'\n",
        ")\n",
        "lda_model.fit(dtm)\n",
        "\n",
        "# Get feature names\n",
        "feature_names_lda = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Display top words per topic\n",
        "n_top_words = 8\n",
        "print(f\"\\nTop {n_top_words} words per topic (LDA, k={n_topics}):\\n\")\n",
        "\n",
        "topic_words_data = []\n",
        "for topic_idx, topic in enumerate(lda_model.components_):\n",
        "    top_indices = topic.argsort()[-n_top_words:][::-1]\n",
        "    top_words = [feature_names_lda[i] for i in top_indices]\n",
        "    top_scores = topic[top_indices]\n",
        "    \n",
        "    print(f\"Topic {topic_idx + 1}:\")\n",
        "    for word, score in zip(top_words, top_scores):\n",
        "        print(f\"  {word}: {score:.4f}\")\n",
        "        topic_words_data.append({\n",
        "            'topic': topic_idx + 1,\n",
        "            'term': word,\n",
        "            'beta': score\n",
        "        })\n",
        "    print()\n",
        "\n",
        "topic_words_df = pd.DataFrame(topic_words_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize topics\n",
        "fig, axes = plt.subplots(1, n_topics, figsize=(18, 6))\n",
        "\n",
        "for topic_idx in range(n_topics):\n",
        "    topic_data = topic_words_df[topic_words_df['topic'] == topic_idx + 1]\n",
        "    topic_data = topic_data.sort_values('beta', ascending=True)\n",
        "    \n",
        "    ax = axes[topic_idx] if n_topics > 1 else axes\n",
        "    ax.barh(topic_data['term'], topic_data['beta'], color='steelblue')\n",
        "    ax.set_title(f'Topic {topic_idx + 1}', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('β (word probability)', fontsize=10)\n",
        "    ax.tick_params(axis='y', labelsize=9)\n",
        "\n",
        "plt.suptitle('Top Words per Topic (LDA)', fontsize=14, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Document-topic distribution\n",
        "doc_topic_dist = lda_model.transform(dtm)\n",
        "docs['dominant_topic'] = doc_topic_dist.argmax(axis=1) + 1\n",
        "\n",
        "print(\"\\nDocument-Topic Distribution Summary:\")\n",
        "print(docs['dominant_topic'].value_counts().sort_index())\n",
        "\n",
        "# Visualize document-topic distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "topic_counts = docs['dominant_topic'].value_counts().sort_index()\n",
        "plt.bar(topic_counts.index, topic_counts.values, color='steelblue', alpha=0.7)\n",
        "plt.xlabel('Topic', fontsize=12)\n",
        "plt.ylabel('Number of Documents', fontsize=12)\n",
        "plt.title('Distribution of Documents Across Topics', fontsize=14, pad=20)\n",
        "plt.xticks(range(1, n_topics + 1))\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n✓ Analysis Complete!\")\n",
        "print(\"=\" * 70)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
