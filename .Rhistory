y = expression(beta ~ "=" ~ P(word ~ "|" ~ topic))
) +
theme_minimal(base_size = 12)
#............... End of Script .........................
ggplot(
terms_per_topic %>%
mutate(term = tidytext::reorder_within(term, beta, topic)),
aes(x = term, y = beta, fill = factor(topic))
) +
geom_col(show.legend = FALSE) +
coord_flip() +
tidytext::scale_x_reordered() +
facet_wrap(~ topic, scales = "free_y") +
labs(
title = "Top words per topic (LDA) Latent Dirichlet Allocation",
x = NULL,
y = expression(beta ~ "=" ~ P(word ~ "|" ~ topic))
) +
theme_minimal(base_size = 12)
# -------------------------
# 0) PACKAGES & SETUP
# -------------------------
# install.packages(c("tidyverse","skimr","tidytext","tokenizers","textstem","text2vec","topicmodels","ggplot2","readxl","data.table","ldatuning","scales","wordcloud","RColorBrewer"))
library(tidyverse)   # wrangling + ggplot2
library(skimr)       # skim() quick EDA
library(tidytext)    # tidy NLP (unnest_tokens, stop_words, tf-idf)
library(tokenizers)  # sentence/word tokenizers
library(textstem)    # lemmatize
library(text2vec)    # BoW/TF-IDF, GloVe
library(topicmodels) # LDA
library(ldatuning)   # NEW: tune number of topics
library(readxl)      # read Excel
library(data.table)  # fread() for GloVe
library(scales)      # NEW: for better number formatting
library(wordcloud)   # NEW: word clouds
library(RColorBrewer) # NEW: color palettes
theme_set(theme_minimal(base_size = 12))
set.seed(123)
# -------------------------
# 1) LOAD DATA
# -------------------------
raw_df <- readxl::read_excel("D:/Projects/xn-project/dataset/FY19_to_FY23_Cleaned.xlsx")
# Keep more context - include Project Name, Resource, Fiscal Year for later analysis
docs <- raw_df %>%
transmute(
doc_id = paste0("D", row_number()),
text   = `Summary Notes`,
project = `Project Name`,
resource = `Resource Name`,
fiscal_year = Fiscal_Year,
billable_hours = `Billable Hours`,
extended_price = `Extended Price`
) %>%
filter(!is.na(text), nzchar(trimws(text)))
glimpse(docs)
skim(docs)
cat("\n=== DATASET OVERVIEW ===\n")
cat("Total documents:", nrow(docs), "\n")
cat("Unique projects:", n_distinct(docs$project), "\n")
cat("Unique resources:", n_distinct(docs$resource), "\n")
cat("Fiscal years:", paste(unique(docs$fiscal_year), collapse = ", "), "\n")
# -------------------------
# 2) ENHANCED EDA
# -------------------------
eda_counts <- docs %>%
mutate(
n_sent = lengths(tokenizers::tokenize_sentences(text)),
n_word = str_count(text, boundary("word")),
n_char = nchar(text)
)
cat("\n=== TEXT LENGTH STATISTICS ===\n")
summary(eda_counts[, c("n_sent", "n_word", "n_char")])
# Distribution of text lengths
ggplot(eda_counts, aes(x = n_word)) +
geom_histogram(bins = 50, fill = "steelblue", alpha = 0.7) +
labs(title = "Distribution of Word Counts in Summary Notes",
x = "Number of Words", y = "Frequency") +
theme_minimal()
# Top tokens (before cleaning)
top_tokens_raw <- docs %>%
mutate(text = str_to_lower(text)) %>%
unnest_tokens(token, text, token = "words") %>%
count(token, sort = TRUE) %>%
slice_head(n = 20)
print(top_tokens_raw)
ggplot(top_tokens_raw %>% slice_head(n = 15),
aes(reorder(token, n), n)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Top 15 Tokens (Raw, Before Cleaning)",
x = NULL, y = "Count") +
theme_minimal()
# -------------------------
# 3) ENHANCED PREPROCESSING
# -------------------------
data(stop_words)
# Create custom stopwords specific to your domain
custom_stopwords <- tibble(
word = c("meeting", "email", "call", "prep", "follow", "review",
"hrs", "hr", "min", "mins", "etc", "re", "1", "2", "3")
) %>%
mutate(lexicon = "custom")
# Combine standard and custom stopwords
all_stopwords <- bind_rows(stop_words, custom_stopwords)
# Enhanced tokenization with bigrams option
tokens <- docs %>%
unnest_tokens(word, text, token = "words") %>%
filter(
!word %in% all_stopwords$word,
str_detect(word, "[a-z]"),
nchar(word) > 2  # Remove very short words
) %>%
mutate(word = textstem::lemmatize_words(word))
# Also create bigrams for context
bigrams <- docs %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(
!word1 %in% all_stopwords$word,
!word2 %in% all_stopwords$word,
str_detect(word1, "[a-z]"),
str_detect(word2, "[a-z]")
) %>%
mutate(
word1 = textstem::lemmatize_words(word1),
word2 = textstem::lemmatize_words(word2)
) %>%
unite(bigram, word1, word2, sep = " ")
# Top bigrams
top_bigrams <- bigrams %>%
count(bigram, sort = TRUE) %>%
slice_head(n = 15)
cat("\n=== TOP BIGRAMS ===\n")
print(top_bigrams)
ggplot(top_bigrams, aes(reorder(bigram, n), n)) +
geom_col(fill = "coral") +
coord_flip() +
labs(title = "Top 15 Bigrams in Summary Notes",
x = NULL, y = "Count") +
theme_minimal()
# -------------------------
# 4) WORD CLOUD VISUALIZATION
# -------------------------
token_counts <- tokens %>%
count(word, sort = TRUE)
# Word cloud
png("wordcloud_summary_notes.png", width = 800, height = 600)
wordcloud(
words = token_counts$word,
freq = token_counts$n,
min.freq = 50,
max.words = 100,
random.order = FALSE,
colors = brewer.pal(8, "Dark2")
)
dev.off()
cat("\n✓ Word cloud saved as 'wordcloud_summary_notes.png'\n")
# -------------------------
# 5) ENCODING (BoW & TF-IDF)
# -------------------------
bow <- tokens %>%
count(doc_id, word, sort = TRUE)
tfidf <- bow %>%
bind_tf_idf(term = word, document = doc_id, n = n) %>%
arrange(desc(tf_idf))
cat("\n=== TOP TF-IDF TERMS (Overall) ===\n")
tfidf_summary <- tfidf %>%
group_by(word) %>%
summarise(
total_tf_idf = sum(tf_idf),
n_docs = n()
) %>%
arrange(desc(total_tf_idf)) %>%
slice_head(n = 20)
print(tfidf_summary)
# Visualize top TF-IDF terms
ggplot(tfidf_summary %>% slice_head(n = 15),
aes(reorder(word, total_tf_idf), total_tf_idf)) +
geom_col(fill = "darkgreen") +
coord_flip() +
labs(title = "Top 15 Terms by Total TF-IDF Score",
subtitle = "Terms that are important but not universally common",
x = NULL, y = "Total TF-IDF") +
theme_minimal()
# TF-IDF by fiscal year (temporal analysis)
tfidf_by_year <- docs %>%
select(doc_id, fiscal_year) %>%
inner_join(tfidf, by = "doc_id") %>%
group_by(fiscal_year, word) %>%
summarise(avg_tf_idf = mean(tf_idf), .groups = "drop") %>%
group_by(fiscal_year) %>%
slice_max(avg_tf_idf, n = 5) %>%
ungroup()
cat("\n=== TOP TERMS BY FISCAL YEAR (TF-IDF) ===\n")
print(tfidf_by_year)
ggplot(tfidf_by_year,
aes(reorder_within(word, avg_tf_idf, fiscal_year),
avg_tf_idf, fill = fiscal_year)) +
geom_col(show.legend = FALSE) +
coord_flip() +
scale_x_reordered() +
facet_wrap(~fiscal_year, scales = "free_y") +
labs(title = "Top Terms per Fiscal Year (TF-IDF)",
x = NULL, y = "Average TF-IDF") +
theme_minimal()
# -------------------------
# 6) PRE-TRAINED WORD EMBEDDINGS (GloVe) - OPTIONAL
# -------------------------
emb_path <- file.path("data", "glove.6B.50d.txt")
if (file.exists(emb_path)) {
glove_df <- fread(emb_path, header = FALSE, quote = "", data.table = FALSE)
rownames(glove_df) <- glove_df[[1]]
glove_df[[1]] <- NULL
emb <- as.matrix(glove_df)
rm(glove_df); invisible(gc())
cat("\n✓ Embeddings loaded:", nrow(emb), "words ×", ncol(emb), "dims\n")
cosine_sim <- function(mat, term, topn = 5) {
q <- tolower(term)
if (!q %in% rownames(mat)) return(tibble(term = character(), sim = numeric()))
sims <- sim2(mat, mat[q, , drop = FALSE], method = "cosine", norm = "l2")[, 1]
tibble(term = names(sims), sim = as.numeric(sims)) %>%
arrange(desc(sim)) %>%
filter(term != q) %>%
slice_head(n = topn)
}
cat("\n=== WORD SIMILARITY EXAMPLES (GloVe) ===\n")
cat("Similar to 'project':\n")
print(cosine_sim(emb, "project", 5))
cat("\nSimilar to 'test':\n")
print(cosine_sim(emb, "test", 5))
} else {
cat("\n⚠ GloVe embeddings not found. Skipping word similarity analysis.\n")
cat("Download from: https://nlp.stanford.edu/projects/glove/\n")
}
# -------------------------
# 7) TOPIC MODELING WITH TUNING
# -------------------------
cat("\n=== PREPARING FOR TOPIC MODELING ===\n")
# Create document-term matrix
dtm <- bow %>%
tidytext::cast_dtm(document = doc_id, term = word, value = n)
cat("DTM dimensions:", nrow(dtm), "documents ×", ncol(dtm), "terms\n")
# Remove sparse terms to speed up computation
dtm_reduced <- removeSparseTerms(dtm, sparse = 0.99)
cat("Reduced DTM:", nrow(dtm_reduced), "documents ×", ncol(dtm_reduced), "terms\n")
# -------------------------
# 7A) FIND OPTIMAL NUMBER OF TOPICS
# -------------------------
cat("\n⏳ Tuning number of topics (this may take a few minutes)...\n")
# Test k from 2 to 10 topics
# Note: This can be slow. Comment out if you want to skip tuning.
topic_tune <- FindTopicsNumber(
dtm_reduced,
topics = seq(2, 10, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 42),
mc.cores = 2,  # Use parallel processing if available
verbose = TRUE
)
cat("\n=== TOPIC NUMBER METRICS ===\n")
print(topic_tune)
# Plot tuning results
FindTopicsNumber_plot(topic_tune) +
labs(title = "Topic Model Tuning Metrics",
subtitle = "Lower is better for Arun2010 & CaoJuan2009; Higher is better for others")
# Based on metrics, select optimal k (you'll adjust based on plot)
optimal_k <- 5  # Adjust this based on your tuning results
# -------------------------
# 7B) FIT LDA WITH OPTIMAL K
# -------------------------
cat(paste0("\n=== FITTING LDA WITH k = ", optimal_k, " ===\n"))
set.seed(42)
lda_model <- topicmodels::LDA(
dtm_reduced,
k = optimal_k,
method = "Gibbs",
control = list(
seed = 42,
iter = 2000,      # More iterations for better convergence
thin = 10,        # Save every 10th iteration
burnin = 500      # Discard first 500 iterations
)
)
# Extract top terms per topic
terms_per_topic <- broom::tidy(lda_model, matrix = "beta") %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
arrange(topic, desc(beta)) %>%
ungroup()
cat("\n=== TOP TERMS PER TOPIC ===\n")
print(terms_per_topic)
# Visualize topics
ggplot(
terms_per_topic %>%
group_by(topic) %>%
slice_max(beta, n = 8) %>%
mutate(term = tidytext::reorder_within(term, beta, topic)),
aes(x = term, y = beta, fill = factor(topic))
) +
geom_col(show.legend = FALSE) +
coord_flip() +
tidytext::scale_x_reordered() +
facet_wrap(~ topic, scales = "free_y", ncol = 2) +
labs(
title = paste0("Top Words per Topic (LDA k=", optimal_k, ")"),
subtitle = "Latent Dirichlet Allocation",
x = NULL,
y = expression(beta ~ "=" ~ P(word ~ "|" ~ topic))
) +
theme_minimal(base_size = 11) +
scale_fill_brewer(palette = "Set2")
# -------------------------
# 7) TOPIC MODELING WITH TUNING
# -------------------------
cat("\n=== PREPARING FOR TOPIC MODELING ===\n")
# Create document-term matrix
dtm <- bow %>%
tidytext::cast_dtm(document = doc_id, term = word, value = n)
cat("DTM dimensions:", nrow(dtm), "documents ×", ncol(dtm), "terms\n")
# Remove sparse terms to speed up computation
dtm_reduced <- removeSparseTerms(dtm, sparse = 0.99)
# -------------------------
# 7) TOPIC MODELING WITH TUNING
# -------------------------
cat("\n=== PREPARING FOR TOPIC MODELING ===\n")
# Create document-term matrix
dtm <- bow %>%
tidytext::cast_dtm(document = doc_id, term = word, value = n)
cat("DTM dimensions:", nrow(dtm), "documents ×", ncol(dtm), "terms\n")
# Remove sparse terms to speed up computation
dtm_reduced <- removeSparseTerms(dtm, sparse = 0.99)
# -------------------------
# 0) PACKAGES & SETUP
# -------------------------
install.packages(c("tidyverse","skimr","tidytext","tokenizers","textstem","text2vec","topicmodels","ggplot2","readxl","data.table","ldatuning","scales","wordcloud","RColorBrewer","tm"))
library(tidyverse)   # wrangling + ggplot2
library(skimr)       # skim() quick EDA
library(readxl)      # read Excel
library(data.table)  # fread() for GloVe
library(scales)      # for better number formatting
library(wordcloud)   # word clouds
library(RColorBrewer) # color palettes
library(tm)          # NEW: for removeSparseTerms and DTM operations
install.packages(c("tidyverse", "skimr", "tidytext", "tokenizers", "textstem", "text2vec", "topicmodels", "ggplot2", "readxl", "data.table", "ldatuning", "scales", "wordcloud", "RColorBrewer", "tm"))
# -------------------------
# 1) LOAD DATA
# -------------------------
raw_df <- readxl::read_excel("D:/Projects/xn-project/dataset/FY19_to_FY23_Cleaned.xlsx")
# Keep more context - include Project Name, Resource, Fiscal Year for later analysis
docs <- raw_df %>%
transmute(
doc_id = paste0("D", row_number()),
text   = `Summary Notes`,
project = `Project Name`,
resource = `Resource Name`,
fiscal_year = Fiscal_Year,
billable_hours = `Billable Hours`,
extended_price = `Extended Price`
) %>%
filter(!is.na(text), nzchar(trimws(text)))
# -------------------------
# 1) LOAD DATA
# -------------------------
raw_df <- readxl::read_excel("D:/Projects/xn-project/dataset/FY19_to_FY23_Cleaned.xlsx")
# Keep more context - include Project Name, Resource, Fiscal Year for later analysis
docs <- raw_df %>%
transmute(
doc_id = paste0("D", row_number()),
text   = `Summary Notes`,
project = `Project Name`,
resource = `Resource Name`,
fiscal_year = Fiscal_Year,
billable_hours = `Billable Hours`,
extended_price = `Extended Price`
) %>%
filter(!is.na(text), nzchar(trimws(text)))
###############################################
## Text Mining in R
## Dataset: FY19_to_FY23_Cleaned.xlsx (Summary Notes column)
## Updated: Enhanced preprocessing, tuning, and visualization
###############################################
# -------------------------
# 0) PACKAGES & SETUP
# -------------------------
install.packages(c("tidyverse","skimr","tidytext","tokenizers","textstem","text2vec","topicmodels","ggplot2","readxl","data.table","ldatuning","scales","wordcloud","RColorBrewer","tm"))
library(tidyverse)   # wrangling + ggplot2
library(skimr)       # skim() quick EDA
library(tidytext)    # tidy NLP (unnest_tokens, stop_words, tf-idf)
library(tokenizers)  # sentence/word tokenizers
library(textstem)    # lemmatize
library(text2vec)    # BoW/TF-IDF, GloVe
library(topicmodels) # LDA
library(ldatuning)   # tune number of topics
install.packages(c("tidyverse", "skimr", "tidytext", "tokenizers", "textstem", "text2vec", "topicmodels", "ggplot2", "readxl", "data.table", "ldatuning", "scales", "wordcloud", "RColorBrewer", "tm"))
# -------------------------
# 1) LOAD DATA
# -------------------------
raw_df <- readxl::read_excel("D:/Projects/xn-project/dataset/FY19_to_FY23_Cleaned.xlsx")
# Keep more context - include Project Name, Resource, Fiscal Year for later analysis
docs <- raw_df %>%
transmute(
doc_id = paste0("D", row_number()),
text   = `Summary Notes`,
project = `Project Name`,
resource = `Resource Name`,
fiscal_year = Fiscal_Year,
billable_hours = `Billable Hours`,
extended_price = `Extended Price`
) %>%
filter(!is.na(text), nzchar(trimws(text)))
glimpse(docs)
skim(docs)
cat("\n=== DATASET OVERVIEW ===\n")
cat("Total documents:", nrow(docs), "\n")
cat("Unique projects:", n_distinct(docs$project), "\n")
cat("Unique resources:", n_distinct(docs$resource), "\n")
cat("Fiscal years:", paste(unique(docs$fiscal_year), collapse = ", "), "\n")
# -------------------------
# 2) ENHANCED EDA
# -------------------------
eda_counts <- docs %>%
mutate(
n_sent = lengths(tokenizers::tokenize_sentences(text)),
n_word = str_count(text, boundary("word")),
n_char = nchar(text)
)
cat("\n=== TEXT LENGTH STATISTICS ===\n")
summary(eda_counts[, c("n_sent", "n_word", "n_char")])
# Distribution of text lengths
ggplot(eda_counts, aes(x = n_word)) +
geom_histogram(bins = 50, fill = "steelblue", alpha = 0.7) +
labs(title = "Distribution of Word Counts in Summary Notes",
x = "Number of Words", y = "Frequency") +
theme_minimal()
# Top tokens (before cleaning)
top_tokens_raw <- docs %>%
mutate(text = str_to_lower(text)) %>%
unnest_tokens(token, text, token = "words") %>%
count(token, sort = TRUE) %>%
slice_head(n = 20)
print(top_tokens_raw)
ggplot(top_tokens_raw %>% slice_head(n = 15),
aes(reorder(token, n), n)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Top 15 Tokens (Raw, Before Cleaning)",
x = NULL, y = "Count") +
theme_minimal()
# -------------------------
# 3) ENHANCED PREPROCESSING
# -------------------------
data(stop_words)
# Create custom stopwords specific to your domain
custom_stopwords <- tibble(
word = c("meeting", "email", "call", "prep", "follow", "review",
"hrs", "hr", "min", "mins", "etc", "re", "1", "2", "3")
) %>%
mutate(lexicon = "custom")
# Combine standard and custom stopwords
all_stopwords <- bind_rows(stop_words, custom_stopwords)
# Enhanced tokenization with bigrams option
tokens <- docs %>%
unnest_tokens(word, text, token = "words") %>%
filter(
!word %in% all_stopwords$word,
str_detect(word, "[a-z]"),
nchar(word) > 2  # Remove very short words
) %>%
mutate(word = textstem::lemmatize_words(word))
# Also create bigrams for context
bigrams <- docs %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(
!word1 %in% all_stopwords$word,
!word2 %in% all_stopwords$word,
str_detect(word1, "[a-z]"),
str_detect(word2, "[a-z]")
) %>%
mutate(
word1 = textstem::lemmatize_words(word1),
word2 = textstem::lemmatize_words(word2)
) %>%
unite(bigram, word1, word2, sep = " ")
# Top bigrams
top_bigrams <- bigrams %>%
count(bigram, sort = TRUE) %>%
slice_head(n = 15)
cat("\n=== TOP BIGRAMS ===\n")
print(top_bigrams)
ggplot(top_bigrams, aes(reorder(bigram, n), n)) +
geom_col(fill = "coral") +
coord_flip() +
labs(title = "Top 15 Bigrams in Summary Notes",
x = NULL, y = "Count") +
theme_minimal()
# -------------------------
# 4) WORD CLOUD VISUALIZATION
# -------------------------
token_counts <- tokens %>%
count(word, sort = TRUE)
# Word cloud
png("wordcloud_summary_notes.png", width = 800, height = 600)
wordcloud(
words = token_counts$word,
freq = token_counts$n,
min.freq = 50,
max.words = 100,
random.order = FALSE,
colors = brewer.pal(8, "Dark2")
)
